from importer import *


class Shape:
    def __init__(self, in_size, out_size, batch_size):
        self.in_size = in_size
        self.out_size = out_size
        self.batch_size = batch_size

    def __str__(self):
        return "(In: " + str(self.in_size) + ", Out: " + str(self.out_size) + ", Batch: " + str(self.batch_size) + ")"


class Input:
    def __init__(self, input, normalize):
        self.input = input
        if normalize:
            norms = tf.norm(self.input, axis=1, keepdims=True)
            self.output = self.input / norms
        else:
            self.output = self.input


class Hyperparameters(dict):
    def __init__(self):
        pass

    def add(self, key: str, value):
        self[key] = value

    def get(self, key: str, default=None):
        if key not in self:
            ratios = {}
            for k in self.keys():
                r = ratio(key, k)
                ratios[r] = k
            max_ratio = max(ratios.keys())
            if max_ratio > 0.7:
                return ratios[max_ratio]
        return self[key]


class Utils:
    def __init__(self):
        pass


class Math(Utils):
    def __init__(self):
        super().__init__()

    @staticmethod
    def cosine_distance(vector1, vector2):
        return sp.spatial.distance.cosine(vector1, vector2)

    @staticmethod
    def are_matrices_identical(A, B):
        """ Generated by Gemini
        Compares two matrices and returns True if they are identical, False otherwise.

        Args:
          A: A list of lists representing the first matrix.
          B: A list of lists representing the second matrix.

        Returns:
          True if the two matrices are identical, False otherwise.
        """

        # Check if the dimensions of the two matrices are equal.
        if len(A) != len(B) or len(A[0]) != len(B[0]):
            return False

        # Compare each element of both matrices.
        for i in range(len(A)):
            for j in range(len(A[0])):
                if A[i][j] != B[i][j]:
                    return False

        # If all elements are equal, return True.
        return True

    @staticmethod
    def raise_matrix_to_power(m, a):
        k = a
        A = m
        result = tf.Variable(A)

        i = tf.constant(0)
        c = lambda i: tf.less(i, k - 1)

        def body(i):
            result.assign(tf.matmul(A, result))
            return [tf.add(i, 1)]

        _ = tf.while_loop(c, body, [i])

        return result

    @staticmethod
    def jaccard(list1, list2):
        # Gemini generated
        intersection = len(list(set(list1).intersection(list2)))
        union = (len(set(list1)) + len(set(list2))) - intersection
        result = float(intersection) / union
        if result > 0.5:
            return True
        else:
            return False


class Statistics(Utils):
    def __init__(self):
        super().__init__()

    @staticmethod
    def t_test(model1, model2):
        statistic, p_value = sp.stats.shapiro(model1)
        print("shapiro-statistic for Model 1 = " + str(statistic))
        print("p-value = " + str(p_value))
        statistic, p_value = sp.stats.shapiro(model2)
        print("shapiro-statistic for Model 2 = " + str(statistic))
        print("p-value = " + str(p_value))
        t_stat, p_value = sp.stats.ttest_rel(model1, model2, alternative="less")
        print("t-statistic = " + str(t_stat))
        print("p-value = " + str(p_value))


class MachineLearning(Utils):
    def __init__(self):
        super().__init__()


class LossFunctions(MachineLearning):
    def __init__(self):
        super().__init__()

    @staticmethod
    def masked_cross_entropy_loss_evaluater_1(predictions, y, mask):
        term1 = tf.boolean_mask(predictions, mask)
        term2 = tf.math.log(term1)
        term3 = tf.boolean_mask(y, mask)
        term4 = term3 * term2
        term5 = tf.reduce_sum(term4)
        term6 = -1 * tf.reduce_mean(term5)
        return term6

    @staticmethod
    def masked_cross_entropy_loss_evaluater_2(predictions, y, mask):
        loss = tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y)
        mask = tf.cast(mask, dtype=tf.float32)
        mask /= tf.reduce_mean(mask)
        loss *= mask
        return tf.reduce_mean(loss)


class Optimizers(MachineLearning):
    def __init__(self):
        super().__init__()

    # Optimizers
    @staticmethod
    def optimizer(learn_rate):
        optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate)
        return optimizer


class Evaluation(MachineLearning):
    def __init__(self):
        super().__init__()

    # Evaluation from Kipf et al 2017
    @staticmethod
    def masked_accuracy_evaluater(prediction, y, mask):
        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))
        accuracy_all = tf.cast(correct_prediction, tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        mask /= tf.reduce_mean(mask)
        accuracy_all *= mask
        accuracy = tf.reduce_mean(accuracy_all)
        return accuracy


class Initializers(MachineLearning):
    def __init__(self):
        super().__init__()

    @staticmethod
    def identity(shape: Shape, axis=1):
        init = tf.keras.initializers.Identity()
        if axis == 1:
            return init(shape=(shape.batch_size, shape.out_size), dtype=tf.float32)
        return init(shape=(shape.in_size, shape.out_size), dtype=tf.float32)

    @staticmethod
    def uniform(shape: Shape, axis=1):
        init = tf.keras.initializers.RandomUniform(0, 1)
        if axis == 1:
            return init(shape=(shape.batch_size, shape.out_size), dtype=tf.float32)
        return init(shape=(shape.in_size, shape.out_size), dtype=tf.float32)

    @staticmethod
    def glorot(shape: Shape, axis=1):
        init = tf.keras.initializers.GlorotUniform()
        if axis == 1:
            return init(shape=(shape.batch_size, shape.out_size), dtype=tf.float32)
        return init(shape=(shape.in_size, shape.out_size), dtype=tf.float32)

    @staticmethod
    def zeros(shape: Shape, axis=1):
        init = tf.keras.initializers.zeros()
        if axis == 1:
            return init(shape=(shape.batch_size, shape.out_size), dtype=tf.float32)
        return init(shape=(shape.in_size, shape.out_size), dtype=tf.float32)

    @staticmethod
    def ones(shape: Shape, axis=1):
        init = tf.keras.initializers.ones()
        if axis == 1:
            return init(shape=(shape.batch_size, shape.out_size), dtype=tf.float32)
        return init(shape=(shape.in_size, shape.out_size), dtype=tf.float32)


class Algorithms(Utils):
    def __init__(self):
        super().__init__()

    # Algorithms
    @staticmethod
    def invert_dict(d):
        inverted_dict = {}
        for key, value_list in d.items():
            for value in value_list:
                inverted_dict.setdefault(value, set()).add(key)
        return inverted_dict

    @staticmethod
    def reverse_dict(d):
        reversed_dict = defaultdict(list)
        for key, value in d.items():
            reversed_dict[value].append(key)
        return reversed_dict

    @staticmethod
    def flip_list(list):
        flipped_edges = []
        for es in list:
            flipped_edges.append((es[1], es[0], es[2]))
        return flipped_edges

    @staticmethod
    def flatten_extend(matrix):
        flat_list = []
        for row in matrix:
            flat_list.extend(row)
        return flat_list

    @staticmethod
    def print_list(l, filename):
        with open(filename, 'w') as fp:
            for item in l:
                # write each item on a new line
                fp.write("%s\n" % item)
            print('Done')

    @staticmethod
    def object_equality(o1, o2):
        if o1 == o2:
            return True
        else:
            return False

    @staticmethod
    def determine_file_encoding(input_file_name):
        encoding = None
        with open(input_file_name, "rb") as rawdata:
            result = chardet.detect(rawdata.read())
            encoding = result["encoding"]
            if encoding == "ascii":
                encoding = "utf-8"
        return encoding

    @staticmethod
    def parse_index_file(filename):
        """Parse index file."""
        index = []
        for line in open(filename):
            index.append(int(line.strip()))
        return index

    @staticmethod
    def sample_mask(idx, l):
        """Create mask."""
        mask = np.zeros(l)
        mask[idx] = 1
        return np.array(mask, dtype=np.bool)

    @staticmethod
    def save_obj(obj, name):
        with open(name + '.pkl', 'wb') as f:
            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

    @staticmethod
    def load_obj(filename):
        with open(filename + '.pkl', 'rb') as f:
            return pickle.load(f)

    @staticmethod
    def save_checkpoint(obj_name, filename):
        print(f"Attempting to save the list to '{filename}'...")
        try:
            self.save_obj(obj_name, filename)
            print(f"Successfully saved the list to '{filename}'")
        except Exception as e:
            print(f"An error occurred while saving: {e}")

        # --- Part 2: Deserializing and Loading the List ---
        print(f"\nAttempting to load the list from '{filename}'...")
        if os.path.exists(filename):
            try:
                loaded_obj = self.load_obj(f)
                print(f"Successfully loaded the list from '{filename}'")
            except Exception as e:
                print(f"An error occurred while loading: {e}")
        else:
            print(f"Error: The file '{filename}' was not found.")

    @staticmethod
    def parse_fasta(filepath):
        """
        Parses a FASTA file and extracts sequence data and descriptions.

        Args:
            filepath: The path to the FASTA file.

        Returns:
            A list of tuples, where each tuple contains (description, sequence).
            Returns an empty list if there's an issue with the file.
        """
        try:
            records = []
            for record in SeqIO.parse(filepath, "fasta"):
                description = record.description  # Header information
                sequence = str(record.seq)  # The protein sequence
                records.append((description, sequence))
            return records
        except FileNotFoundError:
            print(f"Error: File not found at {filepath}")
            return []
        except Exception as e:  # Catching potential other Bio.SeqIO parse errors.
            print(f"An error occurred while parsing the file: {e}")
            return []

    @staticmethod
    def encode_characters(character_sequence):
        ascii_sequence = []
        for c in character_sequence:
            ascii_sequence.append(str(ord(c)))
        return ascii_sequence

    @staticmethod
    def strip_non_alphanumeric(text):
        """Removes all non-alphanumeric characters from a string.

        Args:
          text: The string to strip.

        Returns:
          The stripped string.
        """
        return re.sub(r'[^a-zA-Z0-9]', '', text.rstrip().lstrip())
